{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from glob import glob\n",
    "# from typing import Callable\n",
    "# import torch\n",
    "# from diffusers import DiffusionPipeline\n",
    "# from PIL import Image\n",
    "# from .video_utils import get_video_fps_and_duration, split_video_to_images, combine_images_to_video\n",
    "# from .image_utils import load_image, shrink_image\n",
    "\n",
    "# class VidDiffusionPipeline:\n",
    "# \tdef __init__(self, config: dict):\n",
    "# \t\t'''\n",
    "# \t\tInit\n",
    "# \t\t:param config: config dict\n",
    "# \t\tconfig = {\n",
    "# \t\t\t'pretrained_model_name_or_path': Necessary. Pretrained model name or path of huggingface diffuser\n",
    "# \t\t\t'input_video_path': Necessary. Path to input video.\n",
    "# \t\t\t'output_video_path': Necessary. Path to output video.\n",
    "\t\t\n",
    "# \t\t\t'output_image_dir': Optional. Path to output image directory. Default: /tmp/viddiffusion\n",
    "# \t\t\t'fps': Optional. FPS of output video. Default: None\n",
    "# \t\t\t'duration': Optional. Duration of output video. Default: None\n",
    "# \t\t\t'start_time': Optional. Start time of output video. Default: None\n",
    "# \t\t\t'end_time': Optional. End time of output video. Default: None\n",
    "\n",
    "# \t\t\t'custom_pipeline': Optional. Custom pipeline. Default: lpw_stable_diffusion\n",
    "# \t\t\t'torch_dtype': Optional. Torch dtype. Default: torch.float\n",
    "# \t\t\t'use_auth_token': Optional. Use auth token. Default: None\n",
    "# \t\t\t'device': Optional. Device. Default: 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# \t\t\t'safety_checker': Optional. Safety checker. Default: None\n",
    "# \t\t}\t\n",
    "# \t\t'''\n",
    "\n",
    "# \t\tself.config = config\n",
    "# \t\tself.set_unset_config_values()\n",
    "\n",
    "# \t\tself.pipe = DiffusionPipeline.from_pretrained(\n",
    "# \t\t\t\tpretrained_model_name_or_path=config['pretrained_model_name_or_path'],\n",
    "# \t\t\t\tcustom_pipeline=config['custom_pipeline'],\n",
    "# \t\t\t\ttorch_dtype=config['torch_dtype'],\n",
    "# \t\t\t\tuse_auth_token=config['use_auth_token'],\n",
    "# \t\t).to(config['device'])\n",
    "# \t\tself.pipe.safety_checker = config['safety_checker']\n",
    "\t\n",
    "\n",
    "# \tdef set_config(self, config: dict):\n",
    "# \t\t'''\n",
    "# \t\tSet config dict\n",
    "# \t\t:param config: config dict\n",
    "# \t\t'''\n",
    "\n",
    "# \t\tself.config = config\n",
    "# \t\tself.set_unset_config_values()\n",
    "\n",
    "\n",
    "# \tdef set_unset_config_values(self):\n",
    "# \t\t'''\n",
    "# \t\tSet unset self.config values\n",
    "# \t\t'''\n",
    "# \t\tif self.config['output_image_dir'] is None:\n",
    "# \t\t\tself.config['output_image_dir'] = '/tmp/viddiffusion'\n",
    "\t\t\n",
    "# \t\t# Set video config values\n",
    "# \t\tif self.config['fps'] is None or self.config['duration'] is None:\n",
    "# \t\t\tself.config['fps'], self.config['duration'] = get_video_fps_and_duration(self.config['input_video_path'])\n",
    "# \t\tif self.config['start_time'] is None:\n",
    "# \t\t\tself.config['start_time'] = 0\n",
    "# \t\tif self.config['end_time'] is None:\n",
    "# \t\t\tself.config['end_time'] = self.config['duration']\n",
    "\t\t\n",
    "# \t\t# Set default config values\n",
    "# \t\tfor key, value in self.get_default_config().items():\n",
    "# \t\t\tif self.config[key] is None:\n",
    "# \t\t\t\tself.config[key] = value\n",
    "\n",
    "\n",
    "# \tdef get_default_config(self):\n",
    "# \t\t'''\n",
    "# \t\tGet default config dict\n",
    "# \t\t:return: default config dict\n",
    "# \t\t'''\n",
    "# \t\treturn {\n",
    "# \t\t\t'custom_pipeline': 'lpw_stable_diffusion',\n",
    "# \t\t\t'torch_dtype': torch.float,\n",
    "# \t\t\t'use_auth_token': None,\n",
    "# \t\t\t'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "# \t\t\t'safety_checker': None,\n",
    "# \t\t\t'image_size': 262144,\n",
    "# \t\t\t'is_blend': True,\n",
    "# \t\t\t'strength': 0.58,\n",
    "# \t\t\t'num_inference_steps': 50,\n",
    "# \t\t\t'guidance_scale': 7.5,\n",
    "# \t\t\t'SEED': 2023,\n",
    "# \t\t\t'generater': torch.Generator(\"cuda\").manual_seed(2023),\n",
    "# \t\t\t'blend_alph': 0.27,\n",
    "# \t\t\t'blend_decay_rate': 0.\n",
    "# \t\t}\n",
    "\n",
    "\n",
    "# \tdef vid2images(self):\n",
    "# \t\tsplit_video_to_images(\n",
    "# \t\t\tinput_video_path=self.config['input_video_path'],\n",
    "# \t\t\toutput_dir_path=self.config['output_image_dir'],\n",
    "# \t\t\tfps=self.config['fps'],\n",
    "# \t\t\tstart_time=self.config['start_time'],\n",
    "# \t\t\tend_time=self.config['end_time']\n",
    "# \t\t)\n",
    "\n",
    "\n",
    "# \tdef images2images(self):\n",
    "# \t\t'''\n",
    "# \t\tConvert images to images\n",
    "# \t\tUse self.config['input_image_dir'] and self.config['output_image_dir']\n",
    "# \t\t'''\n",
    "\n",
    "# \t\tif not os.path.exists(self.config['input_image_dir']):\n",
    "# \t\t\traise FileNotFoundError('Input image dir does not exist.')\n",
    "# \t\tif os.path.exists(self.config['output_video_path']):\n",
    "# \t\t\traise FileExistsError('Output video path already exists.')\n",
    "\t\t\n",
    "# \t\timage_paths = sorted(glob.glob(os.path.join(self.config['input_image_dir'], '*.png')))\n",
    "# \t\tfor i_image in range(len(image_paths)):\n",
    "# \t\t\tloaded_image = load_image(image_paths[i_image])\n",
    "# \t\t\tloaded_image = shrink_image(loaded_image, self.config['image_size'])\n",
    "\t\t\t\n",
    "# \t\t\tif self.config['is_blend'] and i_image > 0:\n",
    "# \t\t\t\timage = Image.blend(loaded_image, stack_image, self.config['blend_alph'])\n",
    "# \t\t\t\tblend_alph *= self.config['blend_decay_rate']\n",
    "# \t\t\telse:\n",
    "# \t\t\t\timage = loaded_image.copy()\n",
    "\t\t\t\t\n",
    "# \t\t\timage = self.preprocess_image(image)\n",
    "\t\t\t\n",
    "# \t\t\t# Diffuse image\n",
    "# \t\t\timage = self.pipe.img2img(image=image, prompt=self.config['prompt'], negative_prompt=self.config['negative_prompt'], strength=self.config['strength'], num_inference_steps=self.config['num_inference_steps'], guidance_scale=self.config['guidance_scale'], generator=self.config['generator']).images[0]\n",
    "# \t\t\timage.save(os.path.join(self.config['output_image_dir'], f'{i_image:05d}.png'))\n",
    "\n",
    "# \t\t\tif self.config['is_blend']:\n",
    "# \t\t\t\tstack_image = image.copy()\n",
    "\t\n",
    "\t\n",
    "# \tdef images2vid(self):\n",
    "# \t\tcombine_images_to_video(\n",
    "# \t\t\tinput_dir_path=self.config['input_image_dir'],\n",
    "# \t\t\toutput_video_path=self.config['output_video_path'],\n",
    "# \t\t\tfps=self.config['fps']\n",
    "# \t\t)\n",
    "\n",
    "\n",
    "# \tdef vid2vid(self):\n",
    "# \t\tself.vid2images()\n",
    "# \t\tself.images2images()\n",
    "# \t\tself.images2vid()\n",
    "\t\n",
    "\n",
    "# \tdef images2vid(self):\n",
    "# \t\tself.images2images()\n",
    "# \t\tself.images2vid()\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from viddiffusion import VidDiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\t'input_video_path': 'input.mp4',\n",
    "\t'output_video_path': 'output.mp4',\n",
    "\t'fps': 24,\n",
    "}\n",
    "vid_pipe = VidDiffusionPipeline(config=config)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
